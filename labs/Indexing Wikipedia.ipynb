{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Wikipedia\n",
    "// Written on **September 24th, 2018** <br/>\n",
    "// Status: **Under Development**\n",
    "\n",
    "Goal: Index Wikipedia articles from a search query using wikidata.\n",
    "\n",
    "Plan:\n",
    "- [x] Scrape a decent amount of wikipedia articles to build small stop word repo.\n",
    "- [x] Represent pages as a bag of words, search queries should target top words (excluding stop words)\n",
    "- [ ] Create a table of the headers: \\[word, url, weight\\] using values from `StopWord.values()`\n",
    "- [ ] Wrap with API, create a wikipedia search engine page at `beta.pengra.io`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping\n",
    "import requests\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize.nist import NISTTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a list of \"tags\" for a Wikipedia article\n",
    "\n",
    "`StopWord` is a \"bag of words\" that counts two things: how many articles use a given word and how frequently it's used. This allows me to build a list of \"Stop words,\" a list of words that poorly the context of a wikipedia article because of how often it shows up. Thus, when getting the \"bag of words\" of any other article, words in the `StopWord`'s bag can be removed from that article. This results in a bag of words that describe the article in a meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def query_wiki(wiki_id):\n",
    "    response = requests.get(\"https://data.pengra.io/wikibags/{}/\".format(wiki_id))\n",
    "    try:\n",
    "        return response.json()\n",
    "    except:\n",
    "        return {'bag': {}}\n",
    "\n",
    "def get_random_articles(limit):\n",
    "    response = requests.get(\"https://en.wikipedia.org/w/api.php?action=query&list=random&rnlimit={limit}&rnnamespace=0&format=json\".format(limit=limit))\n",
    "    return [random['id'] for random in response.json()['query']['random']]\n",
    "    \n",
    "\n",
    "class StopWord():\n",
    "    \n",
    "    def __init__(self, iterable, word_threshold=0.0003, article_threshold=0.85):\n",
    "        self._history = []\n",
    "        self._stopwords = {}\n",
    "        self._total = 0\n",
    "        self._total_articles = 0\n",
    "        self.word_threshold = word_threshold\n",
    "        self.article_threshold = article_threshold\n",
    "        self.feed(iterable)\n",
    "        \n",
    "    def feed(self, iterable):\n",
    "        for word in iterable:\n",
    "            if word not in self._history and word != '_random':\n",
    "                self._history.append(word)\n",
    "                bag = query_wiki(word)['bag']\n",
    "                self._total_articles += 1\n",
    "                for token, count in bag.items():\n",
    "                    self._stopwords.setdefault(token, {\"words\": 0, \"articles\": 0})\n",
    "                    self._stopwords[token]['words'] += count\n",
    "                    self._stopwords[token]['articles'] += 1\n",
    "                    self._total += count\n",
    "        \n",
    "    def sort(self, by_words=True):\n",
    "        if by_words:\n",
    "            self._stopwords = {\n",
    "                token: count for token, count in sorted(self._stopwords.items(), key=lambda x: -x[1]['words'])\n",
    "            }\n",
    "        else:\n",
    "            self._stopwords = {\n",
    "                token: count for token, count in sorted(self._stopwords.items(), key=lambda x: -x[1]['articles'])\n",
    "            }\n",
    "        \n",
    "    def values(self, by_words=True):\n",
    "        self.sort(by_words)\n",
    "        stopwords = []\n",
    "        if by_words:\n",
    "            threshold = self._total * self.word_threshold\n",
    "        else:\n",
    "            threshold = self._total_articles * self.article_threshold\n",
    "        for token, counts in self._stopwords.items():\n",
    "            if by_words and counts['words'] >= threshold:\n",
    "                stopwords.append((token, counts['words'] / self._total))\n",
    "            elif not by_words and counts['articles'] >= threshold:\n",
    "                stopwords.append((token, counts['articles'] / self._total_articles))\n",
    "            else:\n",
    "                break\n",
    "        return stopwords\n",
    "        \n",
    "    def clean_bag(self, word, by_words=True):\n",
    "        response = query_wiki(word)\n",
    "        bag = response['bag']\n",
    "        word_count = response['bag_size']\n",
    "        for word, percentage in self.values(by_words):\n",
    "            if word in bag:\n",
    "                del bag[word]\n",
    "        for word, value in bag.items():\n",
    "            bag[word] = value / word_count\n",
    "            \n",
    "        return bag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding `StopWord`s\n",
    "\n",
    "First, `StopWord` is populated with some articles of my liking, then populated with 200 more wikipedia articles (Enough to get a general sense of word usage). The values output shows a list of words and their frequency of usage as a percentage.\n",
    "\n",
    "For instance, take the top 5 non-character words: \"the\", \"of\", \"and\", \"in\" and \"a\". According to the initial scrape below, these words combined make up 11.4% of all words analyzed. This could potentially (I haven't been formally trained in stats yet) imply that the english used on wikipedia is ~11.4% these words. Cross referencing this with [google ngram](https://books.google.com/ngrams/interactive_chart?content=the%2Cof%2Cand%2Cin%2Ca&case_insensitive=on&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t4%3B%2Cthe%3B%2Cc0%3B%2Cs0%3B%3Bthe%3B%2Cc0%3B%3BThe%3B%2Cc0%3B.t1%3B%2Cof%3B%2Cc0%3B.t4%3B%2Cand%3B%2Cc0%3B%2Cs0%3B%3Band%3B%2Cc0%3B%3BAnd%3B%2Cc0%3B.t4%3B%2Cin%3B%2Cc0%3B%2Cs0%3B%3Bin%3B%2Cc0%3B%3BIn%3B%2Cc0%3B.t4%3B%2Ca%3B%2Cc0%3B%2Cs0%3B%3Ba%3B%2Cc0%3B%3BA%3B%2Cc0) yields some similarity in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.05816226525044367),\n",
       " (',', 0.046391015283553626),\n",
       " ('.', 0.04038884327073345),\n",
       " ('of', 0.031022700182766934),\n",
       " ('and', 0.021306916006674965),\n",
       " ('in', 0.020962572510793843),\n",
       " ('a', 0.020194421635366725),\n",
       " ('to', 0.01987656610070723),\n",
       " ('is', 0.011840118666066273),\n",
       " ('(', 0.010558101342939634),\n",
       " (')', 0.010558101342939634),\n",
       " ('was', 0.0087251344264032),\n",
       " ('for', 0.008677456096204276),\n",
       " ('that', 0.008333112600323152),\n",
       " ('as', 0.007570259317140359),\n",
       " ('\"', 0.007024607315974889),\n",
       " ('on', 0.0066537758588721425),\n",
       " ('by', 0.006076338304240723),\n",
       " ('with', 0.005737292400603926),\n",
       " ('at', 0.0047731306121367835),\n",
       " ('he', 0.00468307154398326),\n",
       " ('it', 0.004624798029295685),\n",
       " ('be', 0.004338728048102138),\n",
       " ('from', 0.004248668979948613),\n",
       " ('an', 0.0041903954652610384),\n",
       " ('x', 0.00407384843588589),\n",
       " ('are', 0.0038566471538685667),\n",
       " ('his', 0.0038566471538685667),\n",
       " ('\\\\', 0.003750695308982068),\n",
       " ('this', 0.0033109951527030964),\n",
       " ('{', 0.003003734802532249),\n",
       " ('}', 0.003003734802532249),\n",
       " ('which', 0.0029242709188673746),\n",
       " (':', 0.0028130214817365506),\n",
       " ('were', 0.002532249092787328),\n",
       " ('not', 0.002447487616878129),\n",
       " ('or', 0.0024104044711678544),\n",
       " ('her', 0.002304452626281355),\n",
       " ('has', 0.002219691150372156),\n",
       " ('2', 0.0021561200434402563),\n",
       " ('she', 0.0020607633830424074),\n",
       " ('also', 0.0020342754218207826),\n",
       " ('can', 0.001981299499377533),\n",
       " ('have', 0.001954811538155908),\n",
       " ('one', 0.0019018356157126585),\n",
       " ('function', 0.001854157285513734),\n",
       " ('1', 0.001838264508780759),\n",
       " ('had', 0.001838264508780759),\n",
       " ('if', 0.0017958837708261595),\n",
       " ('war', 0.0017905861785818345),\n",
       " ('but', 0.0017852885863375097),\n",
       " ('such', 0.0017799909940931847),\n",
       " ('they', 0.0017535030328715599),\n",
       " ('its', 0.0017482054406272349),\n",
       " ('s', 0.0016793367414510104),\n",
       " ('first', 0.001626360819007761),\n",
       " ('been', 0.0015998728577861362),\n",
       " ('n', 0.0015574921198315366),\n",
       " ('alex', 0.0015468969353428866),\n",
       " ('after', 0.0014992186051439621),\n",
       " ('their', 0.0014939210128996371),\n",
       " ('all', 0.0014939210128996371),\n",
       " ('when', 0.0014621354594336873),\n",
       " ('=', 0.0014568378671893625),\n",
       " ('black', 0.0014409450904563875),\n",
       " ('displaystyle', 0.001403861944746113),\n",
       " ('space', 0.001377373983524488),\n",
       " ('who', 0.0013561836145471883),\n",
       " ('two', 0.0013243980610812387),\n",
       " ('other', 0.0013032076921039388),\n",
       " ('there', 0.0012608269541493392),\n",
       " ('/', 0.0012608269541493392),\n",
       " ('f', 0.0012449341774163642),\n",
       " ('more', 0.0012396365851720392),\n",
       " (';', 0.0012184462161947394),\n",
       " ('only', 0.0012184462161947394),\n",
       " ('where', 0.0012131486239504146),\n",
       " ('y', 0.0011919582549731148),\n",
       " ('would', 0.0011866606627287898),\n",
       " ('may', 0.0011760654782401398),\n",
       " ('during', 0.0011495775170185152),\n",
       " ('some', 0.0011389823325298652),\n",
       " ('i', 0.0011018991868195905),\n",
       " ('logic', 0.0011018991868195905),\n",
       " ('new', 0.001059518448864991),\n",
       " ('into', 0.001027732895399041),\n",
       " ('then', 0.0010171377109103913),\n",
       " ('h', 0.0010118401186660663),\n",
       " ('many', 0.0009959473419330915),\n",
       " ('about', 0.0009853521574444415),\n",
       " ('used', 0.0009694593807114667),\n",
       " ('these', 0.0009482690117341669),\n",
       " ('lambda', 0.0009429714194898419),\n",
       " ('over', 0.000937673827245517),\n",
       " ('time', 0.000927078642756867),\n",
       " ('army', 0.000916483458268217),\n",
       " ('between', 0.0009111858660238922),\n",
       " ('than', 0.0009005906815352422),\n",
       " ('any', 0.0009005906815352422),\n",
       " ('turing', 0.0008952930892909172),\n",
       " ('-', 0.0008688051280692925),\n",
       " ('theory', 0.0008582099435806426),\n",
       " ('0', 0.0008582099435806426),\n",
       " ('because', 0.0008423171668476677),\n",
       " ('continuous', 0.0008423171668476677),\n",
       " ('number', 0.0008370195746033428),\n",
       " ('c', 0.0008370195746033428),\n",
       " ('office', 0.0008370195746033428),\n",
       " ('post', 0.0008317219823590178),\n",
       " ('system', 0.0008264243901146929),\n",
       " ('hole', 0.000810531613381718),\n",
       " ('called', 0.0007893412444044182),\n",
       " ('most', 0.0007893412444044182),\n",
       " ('set', 0.0007893412444044182),\n",
       " ('known', 0.0007893412444044182),\n",
       " ('however', 0.0007840436521600933),\n",
       " ('while', 0.0007734484676714433),\n",
       " ('so', 0.0007469605064498186),\n",
       " ('up', 0.0007469605064498186),\n",
       " ('d', 0.0007416629142054936),\n",
       " ('will', 0.0007310677297168437),\n",
       " ('no', 0.0007310677297168437),\n",
       " ('[', 0.0007257701374725188),\n",
       " (']', 0.0007257701374725188),\n",
       " ('hilbert', 0.0007257701374725188),\n",
       " ('general', 0.0007151749529838689),\n",
       " ('e', 0.0007151749529838689),\n",
       " ('functions', 0.0007151749529838689),\n",
       " ('him', 0.0007151749529838689),\n",
       " ('out', 0.0007151749529838689),\n",
       " ('example', 0.0007098773607395438),\n",
       " ('work', 0.000704579768495219),\n",
       " ('λ', 0.000704579768495219),\n",
       " ('nikita', 0.000704579768495219),\n",
       " ('both', 0.0006992821762508939),\n",
       " ('service', 0.000688686991762244),\n",
       " ('b', 0.0006833893995179191),\n",
       " ('m', 0.0006833893995179191),\n",
       " ('postal', 0.0006780918072735941),\n",
       " ('form', 0.0006674966227849442),\n",
       " ('later', 0.0006621990305406193),\n",
       " ('under', 0.0006569014382962943),\n",
       " ('every', 0.0006516038460519694),\n",
       " ('being', 0.0006463062538076444),\n",
       " ('mail', 0.0006357110693189945),\n",
       " ('t', 0.0006251158848303446),\n",
       " ('password', 0.0006251158848303446),\n",
       " ('them', 0.0006198182925860196),\n",
       " ('years', 0.0006145207003416947),\n",
       " ('defined', 0.0006145207003416947),\n",
       " ('given', 0.0006092231080973697),\n",
       " ('through', 0.0005986279236087198),\n",
       " ('+', 0.0005986279236087198),\n",
       " ('made', 0.0005933303313643949),\n",
       " ('world', 0.0005933303313643949),\n",
       " ('since', 0.000582735146875745),\n",
       " ('part', 0.000582735146875745),\n",
       " ('season', 0.000582735146875745),\n",
       " ('series', 0.0005668423701427701),\n",
       " ('could', 0.0005615447778984452),\n",
       " ('each', 0.0005562471856541202),\n",
       " ('another', 0.0005562471856541202),\n",
       " ('%', 0.0005562471856541202),\n",
       " ('states', 0.0005456520011654703),\n",
       " ('before', 0.0005456520011654703),\n",
       " ('neptune', 0.0005403544089211454),\n",
       " ('times', 0.0005297592244324955),\n",
       " ('same', 0.0005191640399438456),\n",
       " ('terms', 0.0005191640399438456),\n",
       " ('following', 0.0005138664476995206),\n",
       " ('g', 0.0005085688554551957),\n",
       " ('passwords', 0.0005085688554551957),\n",
       " ('way', 0.0005032712632108706),\n",
       " ('second', 0.0005032712632108706),\n",
       " ('spaces', 0.0005032712632108706),\n",
       " ('until', 0.0004979736709665457),\n",
       " ('we', 0.0004979736709665457),\n",
       " ('holes', 0.0004979736709665457),\n",
       " ('well', 0.0004926760787222207),\n",
       " ('mass', 0.0004926760787222207),\n",
       " ('using', 0.0004873784864778958),\n",
       " ('merman', 0.00048208089423357084),\n",
       " ('now', 0.0004767833019892459),\n",
       " ('university', 0.0004767833019892459),\n",
       " ('v', 0.0004767833019892459),\n",
       " ('u', 0.0004767833019892459),\n",
       " ('air', 0.0004767833019892459),\n",
       " ('amanda', 0.0004767833019892459),\n",
       " ('often', 0.000466188117500596),\n",
       " ('became', 0.000466188117500596),\n",
       " ('order', 0.0004555929330119461),\n",
       " ('state', 0.0004555929330119461),\n",
       " ('field', 0.0004502953407676211),\n",
       " ('use', 0.0004502953407676211),\n",
       " ('point', 0.0004502953407676211),\n",
       " ('said', 0.0004502953407676211),\n",
       " ('what', 0.00044499774852329617),\n",
       " ('even', 0.00044499774852329617),\n",
       " ('group', 0.00044499774852329617),\n",
       " ('calculus', 0.00044499774852329617),\n",
       " ('see', 0.0004397001562789712),\n",
       " ('real', 0.00043440256403464627),\n",
       " ('like', 0.00043440256403464627),\n",
       " ('3', 0.00043440256403464627),\n",
       " ('june', 0.00043440256403464627),\n",
       " ('pope', 0.00043440256403464627),\n",
       " ('three', 0.0004291049717903213),\n",
       " ('product', 0.0004291049717903213),\n",
       " ('against', 0.00042380737954599636),\n",
       " ('much', 0.0004185097873016714),\n",
       " ('show', 0.0004185097873016714),\n",
       " ('theorem', 0.0004185097873016714),\n",
       " ('just', 0.00041321219505734646),\n",
       " ('name', 0.00041321219505734646),\n",
       " ('numbers', 0.00041321219505734646),\n",
       " ('british', 0.00041321219505734646),\n",
       " ('several', 0.0004079146028130215),\n",
       " ('those', 0.0004079146028130215),\n",
       " ('de', 0.0004079146028130215),\n",
       " ('benedict', 0.0004079146028130215),\n",
       " ('family', 0.00040261701056869655),\n",
       " ('neural', 0.0003973194183243716),\n",
       " ('systems', 0.0003973194183243716),\n",
       " ('complex', 0.0003973194183243716),\n",
       " ('−', 0.0003973194183243716),\n",
       " ('school', 0.0003973194183243716),\n",
       " ('division', 0.0003973194183243716),\n",
       " ('very', 0.00039202182608004665),\n",
       " ('found', 0.00039202182608004665),\n",
       " ('day', 0.00039202182608004665),\n",
       " ('do', 0.00038672423383572164),\n",
       " ('p', 0.00038672423383572164),\n",
       " ('possible', 0.00038672423383572164),\n",
       " ('10', 0.00038672423383572164),\n",
       " ('k', 0.0003814266415913967),\n",
       " ('high', 0.0003814266415913967),\n",
       " ('july', 0.0003814266415913967),\n",
       " ('james', 0.00037612904934707173),\n",
       " ('5', 0.00037612904934707173),\n",
       " ('early', 0.0003708314571027468),\n",
       " ('result', 0.0003708314571027468),\n",
       " ('end', 0.0003708314571027468),\n",
       " ('people', 0.0003708314571027468),\n",
       " ('national', 0.0003708314571027468),\n",
       " ('–', 0.0003708314571027468),\n",
       " ('born', 0.0003708314571027468),\n",
       " ('letter', 0.0003708314571027468),\n",
       " ('within', 0.00036553386485842183),\n",
       " ('does', 0.00036553386485842183),\n",
       " ('l', 0.00036553386485842183),\n",
       " ('^', 0.00036553386485842183),\n",
       " (\"'s\", 0.00036553386485842183),\n",
       " ('american', 0.0003602362726140969),\n",
       " ('study', 0.0003602362726140969),\n",
       " ('small', 0.0003602362726140969),\n",
       " ('band', 0.0003602362726140969),\n",
       " ('ross', 0.0003602362726140969),\n",
       " ('term', 0.0003549386803697719),\n",
       " ('including', 0.0003549386803697719),\n",
       " ('back', 0.0003549386803697719),\n",
       " ('re', 0.0003549386803697719),\n",
       " ('september', 0.0003549386803697719),\n",
       " ('based', 0.000344343495881122),\n",
       " ('large', 0.000344343495881122),\n",
       " ('problem', 0.000344343495881122),\n",
       " ('law', 0.000344343495881122),\n",
       " ('military', 0.000344343495881122),\n",
       " ('mathematical', 0.00033904590363679707),\n",
       " ('established', 0.00033904590363679707),\n",
       " ('august', 0.00033904590363679707),\n",
       " ('left', 0.00033904590363679707),\n",
       " ('league', 0.00033904590363679707),\n",
       " ('did', 0.0003337483113924721),\n",
       " ('thus', 0.0003337483113924721),\n",
       " ('definition', 0.0003337483113924721),\n",
       " ('due', 0.0003337483113924721),\n",
       " ('south', 0.0003337483113924721),\n",
       " (\"turing's\", 0.00032845071914814716),\n",
       " ('_', 0.00032845071914814716),\n",
       " ('united', 0.00032845071914814716),\n",
       " ('although', 0.0003231531269038222),\n",
       " ('still', 0.0003231531269038222),\n",
       " ('zeta', 0.0003231531269038222),\n",
       " ('city', 0.0003231531269038222),\n",
       " ('film', 0.0003231531269038222),\n",
       " ('mathematics', 0.00031785553465949726),\n",
       " ('own', 0.00031785553465949726),\n",
       " ('constant', 0.00031785553465949726),\n",
       " ('value', 0.0003125579424151723),\n",
       " ('north', 0.0003125579424151723),\n",
       " ('year', 0.0003125579424151723),\n",
       " ('similar', 0.00030726035017084735),\n",
       " ('case', 0.00030726035017084735),\n",
       " ('make', 0.00030726035017084735),\n",
       " ('operator', 0.00030726035017084735),\n",
       " ('star', 0.00030726035017084735),\n",
       " ('operators', 0.0003019627579265224),\n",
       " ('place', 0.0003019627579265224),\n",
       " ('life', 0.0003019627579265224),\n",
       " ('public', 0.0003019627579265224),\n",
       " ('march', 0.0003019627579265224)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting Pages Example\n",
    "words = StopWord([\n",
    "    \"neural network\", \n",
    "    \"Hilbert_space\", \n",
    "    \"Divisor\", \n",
    "    \"Didactic_method\", \n",
    "    \"Saunders_Mac_Lane\", \n",
    "    \"Password\", \n",
    "    \"Lambda_calculus\",\n",
    "    \"Mathematical_constant\",\n",
    "    \"Riemann_zeta_function\",\n",
    "    \"Continuous_function\",\n",
    "    \"logic\",\n",
    "    \"Neptune\",\n",
    "    \"Black_hole\",\n",
    "    \"Alan_Turing\",\n",
    "    \"War\"\n",
    "])\n",
    "\n",
    "for x in range(2):\n",
    "    words.feed(get_random_articles(100))\n",
    "\n",
    "words.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words by Article Frequency\n",
    "\n",
    "Alternatively, words can be analyzed by how often they appear in articles. For instance, the word \"the\" appears in 89.3% of all articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.9116279069767442),\n",
       " ('the', 0.8930232558139535),\n",
       " ('in', 0.8558139534883721),\n",
       " ('a', 0.8558139534883721)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.values(by_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating the Server\n",
    "\n",
    "The simple endpoint I set up [`data.pengra.io/wikibags`](https://data.pengra.io/wikibags/) is a simple endpoint that reads a wikipedia article and turns it to a bag of words and caches the results. This helps populate the server and redueces DOSing wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_articles(limit):\n",
    "    response = requests.get(\"https://en.wikipedia.org/w/api.php?action=query&list=random&rnlimit={limit}&rnnamespace=0&format=json\".format(limit=limit))\n",
    "    return [random['id'] for random in response.json()['query']['random']]\n",
    "\n",
    "def populate_server(limit):\n",
    "    for x in get_random_articles(limit):\n",
    "        query_wiki(x)\n",
    "\n",
    "populate_server(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "- [ ] Differentiate words between header tags (such as `h1`, `header`, `title`, etc) and body tags\n",
    "- [ ] Categorize different stop words between different subjects, e.g. (Physics based topics vs History based topics will have different stop words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
